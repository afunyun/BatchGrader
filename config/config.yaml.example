# Example configuration file for BatchGrader
# Copy this file to config.yaml and customize as needed

# Batch job settings
max_simultaneous_batches: 2
force_chunk_count: 0  # If >1, forcibly split input into this many chunks regardless of token limits
halt_on_chunk_failure: true  # If true, aborts remaining chunks for a file if any chunk fails critically

# Directory paths (relative to project root)
input_dir: input
output_dir: output
examples_dir: examples/examples.txt

# OpenAI API settings
openai_model_name: gpt-4o-mini-2024-07-18
# Replace with your API key or set via OPENAI_API_KEY environment variable
# openai_api_key: YOUR_OPENAI_API_KEY_HERE

# Batch job settings
poll_interval_seconds: 60
max_tokens_per_response: 1000
response_field: llm_response
batch_api_endpoint: /v1/chat/completions

# Token limits
token_limit: 2000000
split_token_limit: 500000  # Max tokens per split file

# Optional: Set a row limit for split files
# split_row_limit: 10000

# Maximum number of log files to keep before rotating/deleting old ones
max_logs: 50

# Maximum number of archived logs to keep before pruning
max_archive: 5
