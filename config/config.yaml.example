# Example BatchGrader config.yaml
# Directory containing input files (CSV, JSON, or JSONL) to process
input_dir: input

# Directory where output files and logs will be saved
output_dir: output

# Name of the OpenAI model to use for batch processing
openai_model_name: gpt-4o-mini-2024-07-18

# How often (in seconds) to poll the Batch API for job completion
poll_interval_seconds: 30

# Maximum tokens allowed in each LLM response
max_tokens_per_response: 1000

# Name of the field/column in your input data containing the text to evaluate
response_field: assistant_content

# API endpoint for submitting batch jobs (usually don't change)
batch_api_endpoint: /v1/chat/completions

# Path to the examples file used in prompts (relative to project root)
examples_dir: examples/examples.txt

# Maximum number of tokens allowed per day (set according to your OpenAI plan)
token_limit: 2000000

# Maximum number of tokens per input file chunk (used for splitting large files)
split_token_limit: 500000   

# Maximum number of rows per input file chunk (optional; null means unlimited)
split_row_limit: null       

# Maximum number of log files to keep before rotating/deleting old ones
max_logs: 50

# Maximum number of archived logs to keep before pruning
max_archive: 5
