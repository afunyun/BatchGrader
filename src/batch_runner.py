import os
import sys
import datetime
import logging
import tempfile
from pathlib import Path
import asyncio
import pandas as pd
import time
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from .batch_job import BatchJob
from .llm_client import LLMClient
from .rich_display import RichJobTable, print_summary_table
from .logger import logger
from .file_utils import prune_chunked_dir
from .log_utils import prune_logs_if_needed
from .data_loader import load_data, save_data
from .evaluator import load_prompt_template
from .config_loader import load_config, CONFIG_DIR, is_examples_file_default
from .cost_estimator import CostEstimator
from .token_tracker import update_token_log, get_token_usage_for_day, get_token_usage_summary, log_token_usage_event
from .input_splitter import split_file_by_token_limit
from .batch_job import BatchJob
from .rich_display import RichJobTable
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from rich.live import Live
from typing import Optional

LOG_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output', 'logs')
ARCHIVE_DIR = os.path.join(LOG_DIR, 'archive')
prune_logs_if_needed(LOG_DIR, ARCHIVE_DIR)

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


def _generate_chunk_job_objects(original_filepath: str, 
                                system_prompt_content: str, 
                                config: dict, 
                                tiktoken_encoding_func, 
                                response_field: str,  
                                llm_model_name: Optional[str], 
                                api_key_prefix: Optional[str]
                                ) -> list[BatchJob]:
    """Splits the input file using input_splitter.split_file and creates BatchJob objects for each chunk."""
    
    splitter_config = config.get('input_splitter_options', {})
    max_tokens_per_chunk = splitter_config.get('max_tokens_per_chunk', config.get('split_token_limit', 20000))
    max_rows_per_chunk = splitter_config.get('max_rows_per_chunk', config.get('split_row_limit', None))
    force_chunk_count_val = splitter_config.get('force_chunk_count', None)

    logger.debug(f"Generating chunk job objects for: {original_filepath}")
    logger.debug(f"Using splitter config: max_tokens={max_tokens_per_chunk}, max_rows={max_rows_per_chunk}, force_chunks={force_chunk_count_val}")

    if not hasattr(tiktoken_encoding_func, 'encode'):
        logger.error(f"Invalid tiktoken_encoding_func passed. Type: {type(tiktoken_encoding_func)}. Attempting to load default.")
        try:
            import tiktoken
            tiktoken_encoding_func = tiktoken.get_encoding("cl100k_base") 
        except Exception as e:
            logger.critical(f"Failed to load default tiktoken encoder: {e}. Chunking will likely fail.", exc_info=True)
            return []

    def _count_row_tokens(row):
        row_content = " ".join(str(value) for value in row.values if pd.notna(value))
        full_content_for_tokenization = system_prompt_content + "\n" + row_content
        return len(tiktoken_encoding_func.encode(full_content_for_tokenization))

    try:
        chunk_file_paths, _ = split_file_by_token_limit(
            input_path=original_filepath,
            token_limit=max_tokens_per_chunk,
            count_tokens_fn=_count_row_tokens,
            response_field=response_field,
            row_limit=max_rows_per_chunk,
            force_chunk_count=force_chunk_count_val,
            logger=logger
        )
    except Exception as e:
        logger.error(f"Error during file splitting for {original_filepath}: {e}", exc_info=True)
        return [] 

    if not chunk_file_paths:
        logger.warning(f"No chunk files generated by input_splitter for {original_filepath}. The file might be empty or unreadable.")
        return []

    jobs = []
    for i, chunk_path in enumerate(chunk_file_paths):
        try:
            logger.debug(f"Loading chunk file: {chunk_path}")
            chunk_df = load_data(chunk_path)
            if chunk_df is None or chunk_df.empty:
                logger.warning(f"Skipping empty or unreadable chunk file: {chunk_path}")
                continue

            if 'custom_id' not in chunk_df.columns:
                if 'id' in chunk_df.columns:
                    chunk_df = chunk_df.rename(columns={'id': 'custom_id'})
                    logger.info(f"Renamed 'id' to 'custom_id' for chunk: {os.path.basename(chunk_path)}")
                else:
                    logger.warning(f"Neither 'custom_id' nor 'id' column found in chunk: {os.path.basename(chunk_path)}. This may cause issues if LLMClient requires an ID.")
            
            if 'custom_id' in chunk_df.columns:
                if not pd.api.types.is_string_dtype(chunk_df['custom_id']):
                    logger.debug(f"Casting 'custom_id' to string for chunk: {os.path.basename(chunk_path)} because current type is {chunk_df['custom_id'].dtype}")
                    chunk_df['custom_id'] = chunk_df['custom_id'].astype(str)

            job = BatchJob(
                chunk_id_str=os.path.splitext(os.path.basename(chunk_path))[0],
                chunk_df=chunk_df,
                system_prompt=system_prompt_content,
                response_field=response_field, 
                original_filepath=original_filepath, 
                chunk_file_path=chunk_path, 
                llm_model=llm_model_name, 
                api_key_prefix=api_key_prefix, 
                status="pending" 
            )
            jobs.append(job)
        except Exception as e:
            logger.error(f"Error processing chunk file {chunk_path} into BatchJob: {e}", exc_info=True)
            failed_job = BatchJob(
                chunk_id_str=os.path.splitext(os.path.basename(chunk_path))[0] if chunk_path else f"failed_chunk_preparation_{i}",
                chunk_df=None, 
                system_prompt=system_prompt_content,
                response_field=response_field, 
                original_filepath=original_filepath,
                chunk_file_path=chunk_path,
                llm_model=llm_model_name, 
                api_key_prefix=api_key_prefix, 
                status="error", 
                error_message=f"Failed to load/prepare BatchJob from chunk: {e}",
                error_details=traceback.format_exc()
            )
            jobs.append(failed_job)
            
    logger.info(f"Generated {len(jobs)} BatchJob objects from {original_filepath}.")
    return jobs


def _execute_single_batch_job_task(batch_job: BatchJob, llm_client: LLMClient, response_field_name: str):
    """
    Worker function to process a single BatchJob chunk.
    Updates batch_job status and results in place.
    Returns the updated batch_job object.
    """
    # Check if chunk_df is valid before proceeding
    if batch_job.chunk_df is None or batch_job.chunk_df.empty:
        if batch_job.status != "error": # Don't overwrite existing error status from generation
             batch_job.status = "error"
             batch_job.error_message = "Chunk DataFrame is None or empty."
             batch_job.error_details = "Chunk DataFrame was not loaded or was empty when task started."
             logger.error(f"[{batch_job.chunk_id_str}] Skipping task execution: {batch_job.error_message}")
        return batch_job # Return early if no valid data

    try:
        batch_job.status = "running" # Set status only if we have data to process

        # The original check for 'custom_id' can remain, but it's now safe because chunk_df is not None
        if 'custom_id' not in batch_job.chunk_df.columns:
            logger.warning(f"[{batch_job.chunk_id_str}] 'custom_id' column is missing from chunk_df at the start of _execute_single_batch_job_task. This might cause issues with mocks or specific LLMClient implementations.")

        api_result = llm_client.run_batch_job(
            batch_job.chunk_df,
            batch_job.system_prompt,
            response_field_name=response_field_name,
            base_filename_for_tagging=batch_job.chunk_id_str
        )
        if isinstance(api_result, pd.DataFrame):
            batch_job.result_data = api_result
            batch_job.status = "completed"
            logger.debug(f"[{batch_job.chunk_id_str}] Task completed, result is DataFrame.")
        elif isinstance(api_result, dict) and ('error' in api_result or 'custom_id_of_failed_item' in api_result): # Check for error indicators
            batch_job.status = "failed"
            batch_job.error_message = api_result.get('error_message', api_result.get('error', str(api_result)))
            batch_job.error_details = api_result
            batch_job.result_data = None
            logger.warning(f"[{batch_job.chunk_id_str}] Task resulted in failure (dict returned): {batch_job.error_message}")
        else:
            batch_job.status = "error"
            batch_job.error_message = f"Unexpected API result type: {type(api_result)}"
            batch_job.error_details = str(api_result)
            batch_job.result_data = None
            logger.error(f"[{batch_job.chunk_id_str}] Task failed with unexpected API result: {batch_job.error_message}")
    except Exception as exc:
        logger.error(f"[{batch_job.chunk_id_str}] Exception in _execute_single_batch_job_task: {exc}", exc_info=True)
        batch_job.status = "failed"
        batch_job.error_message = str(exc)
        batch_job.error_details = traceback.format_exc()
        batch_job.result_data = None

    return batch_job


def _pfc_submit_jobs(jobs_to_submit, response_field_name, max_workers_config):
    llm_client = LLMClient() 
    with ThreadPoolExecutor(max_workers=max_workers_config) as executor:
        future_to_job_map = {executor.submit(_execute_single_batch_job_task, job, llm_client, response_field_name): job for job in jobs_to_submit}
    return future_to_job_map


def _pfc_handle_job_result(job_from_map, result_job_obj, completed_jobs_list):
    if result_job_obj.status == "running": 
        logger.info(f"Chunk {result_job_obj.chunk_id_str} reported running (final check)")
    elif result_job_obj.status == "completed":
        logger.success(f"Chunk {result_job_obj.chunk_id_str} completed successfully.")
    elif result_job_obj.status == "failed":
        logger.error(f"Chunk {result_job_obj.chunk_id_str} failed: {result_job_obj.error_message}")
    else:
        logger.warning(f"Chunk {result_job_obj.chunk_id_str} finished with unhandled status: {result_job_obj.status}")
    
    if result_job_obj not in completed_jobs_list: 
        completed_jobs_list.append(result_job_obj)


def _pfc_process_completed_future(
    future,
    future_to_job_map,
    completed_jobs_list,
    live_display,
    rich_job_table,
    all_jobs_list,
    halt_on_failure_flag,
    original_filepath,
    llm_output_column_name
):
    """
    Processes a single completed future from the concurrent execution pool.

    Args:
        future: The completed future object.
        future_to_job_map: Dictionary mapping futures to BatchJob objects.
        completed_jobs_list: List to append successfully processed BatchJob objects to.
        live_display: The Rich Live display object to update.
        rich_job_table: The RichJobTable instance used for building the table.
        all_jobs_list: The full list of BatchJob objects for display updates.
        halt_on_failure_flag: Boolean indicating if processing should stop on first failure.
        original_filepath: Path of the original file being processed (for logging).
        llm_output_column_name: Name of the column for LLM output.
    Returns:
        bool: True if a failure occurred and halt_on_failure is set, otherwise False.
    """
    job_from_future = future_to_job_map[future]
    logger.info(f"Future completed for chunk {job_from_future.chunk_id_str}. Processing result...")
    
    try:
        processed_job_in_task = future.result() 

        job_from_future.status = processed_job_in_task.status
        job_from_future.error_message = processed_job_in_task.error_message
        job_from_future.error_details = processed_job_in_task.error_details
        job_from_future.result_data = processed_job_in_task.result_data

        logger.info(f"[{job_from_future.chunk_id_str}] Processed job status from task: {job_from_future.status}")
        logger.debug(f"[{job_from_future.chunk_id_str}] Result data from task (type {type(job_from_future.result_data)}): {str(job_from_future.result_data)[:200]}")

        if job_from_future.status == "completed":
            if isinstance(job_from_future.result_data, pd.DataFrame):
                completed_jobs_list.append(job_from_future)
                logger.success(f"Chunk {job_from_future.chunk_id_str} completed successfully. DataFrame stored in BatchJob.")
            else:
                job_from_future.status = "error" 
                job_from_future.error_message = f"Completed status but result_data is not DataFrame (type: {type(job_from_future.result_data)})"
                logger.error(f"Chunk {job_from_future.chunk_id_str}: {job_from_future.error_message}")

        if job_from_future.status == "failed" or job_from_future.status == "error":
            logger.error(f"Chunk {job_from_future.chunk_id_str} reported as {job_from_future.status}. Error: {job_from_future.error_message}")
            if halt_on_failure_flag:
                logger.error(f"[HALT] {job_from_future.status.capitalize()} detected in chunk {job_from_future.chunk_id_str}. Halting.")
                return True
            else:
                error_custom_id = None
                if isinstance(job_from_future.error_details, dict):
                    error_custom_id = job_from_future.error_details.get('custom_id', job_from_future.error_details.get('custom_id_of_failed_item'))
                
                if error_custom_id is None and job_from_future.chunk_df is not None and 'custom_id' in job_from_future.chunk_df.columns:
                    error_custom_id = job_from_future.chunk_id_str
                elif error_custom_id is None:
                    error_custom_id = job_from_future.chunk_id_str

                error_df_data = {
                    'custom_id': error_custom_id,
                    llm_output_column_name: f"ERROR: {job_from_future.error_message}",
                    'error_type': job_from_future.status.capitalize() + 'Error',
                    'original_file': original_filepath,
                    'chunk_id': job_from_future.chunk_id_str
                }
                if isinstance(job_from_future.error_details, dict):
                    error_df_data['error_details'] = str(job_from_future.error_details)

                job_from_future.result_data = pd.DataFrame([error_df_data])
                completed_jobs_list.append(job_from_future)
                logger.warning(f"Chunk {job_from_future.chunk_id_str} {job_from_future.status} but continuing. BatchJob (with error info) added.")

    except Exception as e:
        job_from_future.status = "error" 
        job_from_future.error_message = f"Exception processing future for {job_from_future.chunk_id_str}: {e}"
        job_from_future.error_details = traceback.format_exc()
        logger.error(f"[{job_from_future.chunk_id_str}] Exception in _pfc_process_completed_future: {e}", exc_info=True)
        
        if not halt_on_failure_flag:
            error_df = pd.DataFrame([{
                'custom_id': job_from_future.chunk_id_str,
                llm_output_column_name: f"ERROR: {job_from_future.error_message}",
                'error_type': 'FutureProcessingError',
                'original_file': original_filepath,
                'chunk_id': job_from_future.chunk_id_str,
                'error_details': traceback.format_exc()
            }])
            job_from_future.result_data = error_df
            completed_jobs_list.append(job_from_future)
            logger.error(f"Chunk {job_from_future.chunk_id_str} had future processing error but continuing. BatchJob (with error info) added.")
        elif halt_on_failure_flag:
            logger.error(f"[HALT] Future processing error for {job_from_future.chunk_id_str}. Halting.")
            return True 

    if live_display: live_display.update(rich_job_table.build_table(all_jobs_list))
    
    if (job_from_future.status == "failed" or job_from_future.status == "error") and halt_on_failure_flag:
        return True 

    return False 

def _pfc_aggregate_and_cleanup(completed_jobs_list: list[BatchJob], 
                               original_filepath: str,
                               response_field_name: str) -> Optional[pd.DataFrame]:
    """Aggregates results from completed jobs and cleans up chunked files."""
    all_results_dfs = []
    total_processed_rows = 0 # This will now count only successfully processed rows for the log message
    original_file_stem = Path(original_filepath).stem

    for job in completed_jobs_list:
        if job.status == "completed" and job.result_data is not None and not job.result_data.empty:
            all_results_dfs.append(job.result_data)
            total_processed_rows += len(job.result_data)
        elif job.status == "failed" and job.chunk_df is not None and not job.chunk_df.empty:
            # This job failed, but we want to include its original rows with an error message
            # when not halting on failure.
            failed_chunk_df_copy = job.chunk_df.copy()
            failed_chunk_df_copy[response_field_name] = job.error_message 
            all_results_dfs.append(failed_chunk_df_copy)
            logger.info(f"Job {job.chunk_id_str} failed. Original data with error message added to final output.")
        elif job.status == "error" and job.chunk_df is not None and not job.chunk_df.empty: # e.g. prep error
            # Also include rows for jobs that had errors during BatchJob preparation phase, if they have a chunk_df
            error_chunk_df_copy = job.chunk_df.copy()
            error_chunk_df_copy[response_field_name] = job.error_message or "Error during job preparation"
            all_results_dfs.append(error_chunk_df_copy)
            logger.info(f"Job {job.chunk_id_str} had preparation error. Original data with error message added to final output.")


    if not all_results_dfs:
        logger.error(f"[FAILURE] All chunks failed or yielded no results for {os.path.basename(original_filepath)}. No aggregated file produced.")
        chunked_dir_path = os.path.join(os.path.dirname(original_filepath), '_chunked')
        if os.path.exists(chunked_dir_path):
            prune_chunked_dir(chunked_dir_path)
        return None

    combined_df = pd.concat(all_results_dfs, ignore_index=True)
    logger.success(f"Results aggregated for {os.path.basename(original_filepath)}. {total_processed_rows} rows processed.")
    # Ensure original 'id' column is present if 'custom_id' was used
    if 'custom_id' in combined_df.columns and 'id' not in combined_df.columns:
        combined_df['id'] = combined_df['custom_id']
        logger.debug(f"Added 'id' column to aggregated results, copied from 'custom_id'.")
    
    chunked_dir_path = os.path.join(os.path.dirname(original_filepath), '_chunked')
    if os.path.exists(chunked_dir_path): 
        prune_chunked_dir(chunked_dir_path)
    return combined_df

def process_file_concurrently(filepath, config, system_prompt_content, response_field, llm_model_name, api_key_prefix, tiktoken_encoding_func):
    jobs = _generate_chunk_job_objects(
        original_filepath=filepath, 
        system_prompt_content=system_prompt_content, 
        config=config, 
        tiktoken_encoding_func=tiktoken_encoding_func,
        response_field=response_field,  
        llm_model_name=llm_model_name, 
        api_key_prefix=api_key_prefix 
    )
    if not jobs:
        logger.info(f"No data loaded from {filepath}. Skipping.")
        return None
    halt_on_failure = config.get('halt_on_chunk_failure', True)
    completed_jobs = []
    rich_table = RichJobTable() 
    failure_detected = False

    future_to_job = _pfc_submit_jobs(jobs, response_field, config.get('max_simultaneous_batches', 2))

    try:
        llm_output_column_name = config.get('llm_output_column_name', 'llm_response')
        with Live(rich_table.build_table(jobs), console=rich_table.console, refresh_per_second=5) as live:
            for future in as_completed(future_to_job):
                failure_detected = _pfc_process_completed_future(
                    future,
                    future_to_job,
                    completed_jobs,
                    live,
                    rich_table,
                    jobs,
                    halt_on_failure,
                    filepath,
                    llm_output_column_name
                )
                if failure_detected:
                    break
            
            if failure_detected:
                cancelled_count = 0
                for fut_to_cancel in future_to_job:
                    if not fut_to_cancel.done():
                        if fut_to_cancel.cancel():
                            cancelled_count += 1
                if cancelled_count > 0:
                    logger.warning(f"Cancelled {cancelled_count} pending chunk jobs due to halt on failure for {os.path.basename(filepath)}.")
        
        logger.info(f"All chunks processed for {os.path.basename(filepath)}. Aggregating results...")
        return _pfc_aggregate_and_cleanup(completed_jobs, filepath, response_field)
    finally:
        if jobs:
            pass


def process_file(filepath, output_dir):
    """
    Processes a single input file using the OpenAI Batch API workflow via LLMClient.
    Loads data, prepares batch requests, manages the batch job, processes results, and saves output.
    Handles errors and logs appropriately.
    Enforces configured token limit per batch, halts and warns if exceeded, and logs daily submitted tokens per API key (censored) in output/token_usage_log.json.
    Returns:
        True if successful, False if any error or batch job failure.
    """
    filename = os.path.basename(filepath)
    file_root, file_ext = os.path.splitext(filename)
    if 'tests' in filepath.replace('\\', '/').lower():
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
        output_dir = os.path.join(project_root, 'tests', 'output')
    else:
        output_dir = output_dir
    os.makedirs(output_dir, exist_ok=True)
    config_force_chunk = config.get('force_chunk_count', 0) 
    if 'legacy' in file_root.lower():
        out_suffix = '_results'
    elif config_force_chunk and config_force_chunk > 1:
        out_suffix = '_forced_results'
    else:
        out_suffix = '_results'
    output_filename = f"{file_root}{out_suffix}{file_ext}"
    output_path = os.path.join(output_dir, output_filename)
    if os.path.exists(output_path):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"{file_root}{out_suffix}_{timestamp}{file_ext}"
        output_path = os.path.join(output_dir, output_filename)
        logger.info(f"Output file already exists. Using new filename: {output_filename}")

    logger.info(f"Processing file: {filepath}")
    df = None
    try:
        df = load_data(filepath)
        logger.info(f"Loaded {len(df)} rows from {filepath}")

        examples_dir = config.get('examples_dir')
        if not examples_dir:
            raise ValueError("'examples_dir' not found in config.yaml.")
        project_root = CONFIG_DIR.parent
        abs_examples_path = (project_root / examples_dir).resolve()
        if is_examples_file_default(abs_examples_path):
            system_prompt_content = load_prompt_template("batch_evaluation_prompt_generic")
        else:
            if not abs_examples_path.exists():
                raise FileNotFoundError(f"Examples file not found: {abs_examples_path}. Please provide a valid examples file or update your config.")
            with open(abs_examples_path, 'r', encoding='utf-8') as ex_f:
                example_lines = [line.strip() for line in ex_f if line.strip()]
            if not example_lines:
                raise ValueError(f"No examples found in {abs_examples_path}.")
            formatted_examples = '\n'.join(f"- {ex}" for ex in example_lines)
            system_prompt_template = load_prompt_template("batch_evaluation_prompt")
            if '{dynamic_examples}' not in system_prompt_template:
                raise ValueError("'{dynamic_examples}' placeholder missing in prompt template.")
            system_prompt_content = system_prompt_template.format(dynamic_examples=formatted_examples)

        try:
            import tiktoken
            model_name = config.get('openai_model_name', 'gpt-4o-mini-2024-07-18')
            try:
                enc = tiktoken.encoding_for_model(model_name)
            except KeyError:
                logger.warning(f"[WARN] tiktoken does not recognize model '{model_name}', using cl100k_base encoding.")
                enc = tiktoken.get_encoding("cl100k_base")
        except Exception:
            logger.error("\n\n############################")
            logger.error("ERROR: You need tiktoken or half of the functionality explodes, my guy!")
            logger.error("RERUN: uv pip install -r requirements.txt")
            logger.error("############################\n\n")
            raise RuntimeError("You need tiktoken or half of the functionality explodes my guy. Run 'uv pip install -r requirements.txt'")

        def count_input_tokens_per_row(row, system_prompt_content, response_field, enc):
            sys_tokens = len(enc.encode(system_prompt_content))
            user_prompt = f"Please evaluate the following text: {str(row[response_field])}"
            user_tokens = len(enc.encode(user_prompt))
            return sys_tokens + user_tokens

        def count_completion_tokens(row, enc):
            completion = row.get('llm_score', '')
            return len(enc.encode(str(completion)))

        def count_submitted_tokens(row):
            sys_tokens = len(enc.encode(system_prompt_content))
            user_prompt = f"Please evaluate the following text: {str(row[RESPONSE_FIELD])}"
            user_tokens = len(enc.encode(user_prompt))
            return sys_tokens + user_tokens
        
        token_counts = df.apply(count_submitted_tokens, axis=1)
        total_tokens = token_counts.sum()
        avg_tokens = token_counts.mean()
        max_tokens = token_counts.max()
        logger.info(f"[SUBMITTED TOKENS] Total: {total_tokens}, Avg: {avg_tokens:.1f}, Max: {max_tokens}")

        if total_tokens > TOKEN_LIMIT:
            logger.error(f"[ERROR] Total submitted tokens ({total_tokens}) exceeds the allowed cap of {TOKEN_LIMIT} for a single batch.")
            logger.error("Please reduce your batch size or check your usage at https://platform.openai.com/usage.")
            logger.error("Batch submission halted. No API calls were made.")
            try:
                llm_client = LLMClient()
                update_token_log(llm_client.api_key, 0)
            except Exception as e:
                logger.error(f"[WARN] Could not log token usage: {e}")
            return False
        try:
            llm_client = LLMClient()
            update_token_log(llm_client.api_key, int(total_tokens))
        except Exception as e:
            logger.error(f"[WARN] Could not log token usage: {e}")

        if df.empty:
            logger.info(f"No data loaded from {filepath}. Skipping.")
            return False

        MAX_BATCH_SIZE = 50000
        if len(df) > MAX_BATCH_SIZE:
            logger.warning(f"[WARN] Input file contains {len(df)} rows. Only the first {MAX_BATCH_SIZE} will be sent to the API (limit is 50,000 per batch). The rest will be ignored for this run. Simultaneous requests to the API are not supported yet but I am working on it.")
            df = df.iloc[:MAX_BATCH_SIZE].copy() 

        force_chunk_count = config.get('force_chunk_count', 0)
        if force_chunk_count > 1 or config.get('split_token_limit', 500_000) < total_tokens:
            df_with_results = process_file_concurrently(
                filepath, config, system_prompt_content, RESPONSE_FIELD, model_name, llm_client.api_key, enc
            )
        else:
            llm_client = LLMClient()
            try:
                df_with_results = llm_client.run_batch_job(
                    df, system_prompt_content, response_field_name=RESPONSE_FIELD, base_filename_for_tagging=filename
                )
            except Exception as batch_exc:
                logger.error(f"[ERROR] Batch job failed for {filepath}: {batch_exc}")
                return False

        save_data(df_with_results.drop(columns=['custom_id'], errors='ignore'), output_path)
        logger.success(f"Processed {filepath}. Results saved to {output_path}")
        logger.success(f"Total rows successfully processed: {len(df_with_results)}")
        error_rows = df_with_results['llm_score'].str.contains('Error', case=False)
        if error_rows.any():
            logger.info(f"Total rows with errors: {error_rows.sum()}")
            if error_rows.sum() == len(df_with_results):
                logger.error(f"[BATCH FAILURE] All rows failed for {filepath}. Halting further processing.")
                return False
        
        try:
            input_col_name = 'input_tokens'
            output_col_name = 'output_tokens'
            if enc is None and (input_col_name not in df_with_results.columns or output_col_name not in df_with_results.columns):
                logger.error("\n\n############################")
                logger.error("ERROR: You need tiktoken or half of the functionality explodes, my guy!")
                logger.error("RERUN: uv pip install -r requirements.txt")
                logger.error("############################\n\n")
                raise RuntimeError("You need tiktoken or half of the functionality explodes my guy. Run 'uv pip install -r requirements.txt'")
            if enc is not None and (input_col_name not in df_with_results.columns or output_col_name not in df_with_results.columns):
                df_with_results[input_col_name] = df_with_results.apply(lambda row: count_input_tokens_per_row(row, system_prompt_content, RESPONSE_FIELD, enc), axis=1)
                df_with_results[output_col_name] = df_with_results.apply(lambda row: count_completion_tokens(row, enc), axis=1)
            n_input_tokens = int(df_with_results[input_col_name].sum()) if input_col_name in df_with_results.columns else 0
            n_output_tokens = int(df_with_results[output_col_name].sum()) if output_col_name in df_with_results.columns else 0
            model_name = config.get('openai_model_name', 'gpt-4o-2024-08-06')
            try:
                cost = CostEstimator.estimate_cost(model_name, n_input_tokens, n_output_tokens)
                logger.info(f"Estimated LLM cost: ${cost:.4f} (input: {n_input_tokens} tokens, output: {n_output_tokens} tokens, model: {model_name})")
            except Exception as ce:
                logger.error(f"Could not estimate cost: {ce}")
            try:
                log_token_usage_event(
                    api_key=llm_client.api_key,
                    model=model_name,
                    input_tokens=n_input_tokens,
                    output_tokens=n_output_tokens,
                    timestamp=None,
                    request_id=None
                )
                logger.info("Token usage event logged to output/token_usage_events.jsonl.")
            except Exception as log_exc:
                logger.error(f"[Token Logging Error] Failed to log token usage event: {log_exc}")
        except Exception as cost_exception:
            logger.error(f"[Cost Estimation Error] {cost_exception}")

    except Exception as e:
        logger.error(f"An error occurred while processing {filepath}: {e}")
        log_basename = os.path.splitext(filename)[0] + ".log"
        log_base_path = os.path.join(output_dir, log_basename)
        if os.path.exists(log_base_path):
            file_root, file_ext = os.path.splitext(log_basename)
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            new_log_filename = f"{file_root}_{timestamp}{file_ext}"
            log_path = os.path.join(output_dir, new_log_filename)
            logger.info(f"Log file already exists. Using new filename: {new_log_filename}")
        else:
            log_path = log_base_path
        try:
            with open(log_path, 'w') as f_log:
                f_log.write(f"Failed to process {filepath}.\nError: {e}\n")
            logger.info(f"Logged error to {log_path}")
        except Exception as save_err:
            logger.error(f"Double fail: exception in processing and then in saving error log for {filepath}: {save_err} ... aborting.")
        return False

    return True

def get_request_mode(args):
    """
    Indicates in the CLI whether or not requests are being submitted or we're in a safe count/split mode.
    """
    if getattr(args, 'count_tokens', False) or getattr(args, 'split_tokens', False):
        return "Split/Count (NO REQUESTS MADE)"
    return "API Request/Batch"

def print_token_cost_stats():
    """
    Prints token/cost usage stats (all time, today, per-model breakdown) using token_tracker utilities.
    """
    from datetime import datetime
    today = datetime.now().strftime('%Y-%m-%d')
    logger.info("\n================= TOKEN USAGE & COST STATS =================")
    logger.info("ALL TIME:")
    summary_all = get_token_usage_summary()
    print_token_cost_summary(summary_all)
    logger.info("\nTODAY:")
    summary_today = get_token_usage_summary(start_date=today, end_date=today)
    print_token_cost_summary(summary_today)
    logger.info("===========================================================\n")

def print_token_cost_summary(summary):
    total_tokens = summary.get('total_tokens', 0)
    total_cost = summary.get('total_cost', 0.0)
    breakdown = summary.get('breakdown', {})
    logger.info(f"  Total tokens: {total_tokens:,}")
    logger.info(f"  Total cost: ${total_cost:,.6f}")
    if breakdown:
        logger.info("  Per-model breakdown:")
        logger.info("    Model           | Tokens      | Cost      | Count")
        logger.info("    --------------- | ----------- | --------- | -----")
        for model, stats in breakdown.items():
            tokens = stats.get('tokens', 0)
            cost = stats.get('cost', 0.0)
            count = stats.get('count', 0)
            logger.info(f"    {model:<15} | {tokens:>11,} | ${cost:>8,.4f} | {count:>5}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="BatchGrader Runner")
    parser.add_argument('--log_dir', type=str, default=None, help='Directory for log files (default: output/logs or as set by test runner)')
    args, unknown = parser.parse_known_args()
    if args.log_dir:
        logger = BatchGraderLogger(log_dir=args.log_dir)
    parser = argparse.ArgumentParser(description="BatchGrader CLI: batch LLM evaluation, token counting, and input splitting.")
    parser.add_argument('--count-tokens', action='store_true', help='Count tokens in input file(s) and print stats.')
    parser.add_argument('--split-tokens', action='store_true', help='Split input file(s) into parts not exceeding the configured token limit.')
    parser.add_argument('--file', type=str, default=None, help='Only process the specified file in the input directory.')
    parser.add_argument('--config', type=str, default=None, help='Path to alternate config YAML file (default: config/config.yaml).')
    parser.add_argument('--costs', action='store_true', help='Show token/cost usage stats and exit.')
    parser.add_argument('--statistics', action='store_true', help='Show API usage stats even in count/split modes.')

    if '--file' in sys.argv:
        try:
            file_arg_index = sys.argv.index('--file')
            if file_arg_index + 1 >= len(sys.argv) or sys.argv[file_arg_index + 1].startswith('--'):
                logger.error("usage: batch_runner.py [-h] [--count-tokens] [--split-tokens] [--file FILE] [--costs] [--statistics]")
                logger.error("batch_runner.py: error: argument --file: expected one argument (the filename). It must be placed immediately after --file.")
                logger.error("Example: python batch_runner.py --file my_data.csv")
                sys.exit(2)
        except ValueError: 
            logger.error("severe oof error: basically something is COOKED if this happens") 
            pass

    args = parser.parse_args()
    config = load_config(args.config)
    PROJECT_ROOT = Path(__file__).resolve().parent.parent
    INPUT_DIR = str(PROJECT_ROOT / config['input_dir'])
    OUTPUT_DIR = str(PROJECT_ROOT / config['output_dir'])
    RESPONSE_FIELD = config['response_field']
    TOKEN_LIMIT = config.get('token_limit', 2_000_000)
    split_token_limit = config.get('split_token_limit', 500_000)
    model_name = config['openai_model_name']

    os.makedirs(INPUT_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    show_stats = args.statistics or (not args.count_tokens and not args.split_tokens)

    llm_client = LLMClient()
    if not llm_client.api_key:
        logger.error("Error: OPENAI_API_KEY not set in config/config.yaml.")
        exit(1)
    else:
        tokens_today = get_token_usage_for_day(llm_client.api_key)
        if show_stats:
            logger.info("\n================= API USAGE =================")
            logger.info(f"TOTAL TOKENS SUBMITTED TODAY: {tokens_today:,}")
            logger.info(f"TOKEN LIMIT: {TOKEN_LIMIT}")
            logger.info(f"TOKENS REMAINING: {TOKEN_LIMIT - tokens_today:,}")
            logger.info(f"SPLIT TOKEN LIMIT: {split_token_limit}")
            logger.info(f"System is running in {get_request_mode(args)} mode.")
            logger.info("==============================================\n")

    if getattr(args, 'costs', False):
        print_token_cost_stats()
        exit(0)

    logger.info(f"Valid INPUT_DIR: {INPUT_DIR}")
    logger.info(f"Valid OUTPUT_DIR: {OUTPUT_DIR}")

    from pathlib import Path
    def resolve_and_load_input_file(file_arg):
        """
        Resolves the file path for CLI input and loads the data.
        - Absolute path: used as-is
        - Relative with directory: resolved from project root
        - Bare filename: resolved from input dir
        Returns (resolved_path, DataFrame)
        """
        file_arg_path = Path(file_arg)
        if file_arg_path.is_absolute():
            resolved_path = str(file_arg_path)
        elif file_arg_path.parent != Path('.'):
            resolved_path = str((PROJECT_ROOT / file_arg_path).resolve())
        else:
            resolved_path = os.path.join(INPUT_DIR, file_arg)
        if not os.path.exists(resolved_path):
            logger.error(f"File {file_arg} not found at {resolved_path}.")
            logger.error("Halting: Missing input file.")
            exit(1)
        df = load_data(resolved_path)
        return resolved_path, df

    files_found = []
    if args.file:
        resolved_path, df = resolve_and_load_input_file(args.file)
        files_found = [(resolved_path, df)]
    else:
        files_found = []
        for file_to_process in os.listdir(INPUT_DIR):
            if file_to_process.endswith((".csv", ".json", ".jsonl")):
                resolved_path, df = resolve_and_load_input_file(file_to_process)
                files_found.append((resolved_path, df))
    if not files_found:
        logger.info(f"Nothing found in {INPUT_DIR} (looked for .csv, .json, .jsonl, if your data isn't in one of these formats please reformat.)")
        exit(0)

    def get_token_counter(system_prompt_content, response_field, enc):
        def count_submitted_tokens(row):
            sys_tokens = len(enc.encode(system_prompt_content))
            user_prompt = f"Please evaluate the following text: {str(row[response_field])}"
            user_tokens = len(enc.encode(user_prompt))
            return sys_tokens + user_tokens
        return count_submitted_tokens

    for resolved_path, df in files_found:
        try:
            logger.info(f"\nProcessing file: {resolved_path}")
            
            examples_dir = config.get('examples_dir')
            if not examples_dir:
                raise ValueError("'examples_dir' not found in config.yaml.")
            project_root = CONFIG_DIR.parent
            abs_examples_path = (project_root / examples_dir).resolve()
            if is_examples_file_default(abs_examples_path):
                system_prompt_content = load_prompt_template("batch_evaluation_prompt_generic")
            else:
                if not abs_examples_path.exists():
                    raise FileNotFoundError(f"Examples file not found: {abs_examples_path}. Please provide a valid examples file or update your config.")
                with open(abs_examples_path, 'r', encoding='utf-8') as ex_f:
                    example_lines = [line.strip() for line in ex_f if line.strip()]
                if not example_lines:
                    raise ValueError(f"No examples found in {abs_examples_path}.")
                formatted_examples = '\n'.join(f"- {ex}" for ex in example_lines)
                system_prompt_template = load_prompt_template("batch_evaluation_prompt")
                if '{dynamic_examples}' not in system_prompt_template:
                    raise ValueError("'{dynamic_examples}' placeholder missing in prompt template.")
                system_prompt_content = system_prompt_template.format(dynamic_examples=formatted_examples)

            try:
                import tiktoken
                enc = tiktoken.encoding_for_model(config.get('openai_model_name', 'gpt-4o-mini-2024-07-18'))
            except Exception:
                logger.error("\n\n############################")
                logger.error("ERROR: You need tiktoken or half of the functionality explodes, my guy!")
                logger.error("RERUN: uv pip install -r requirements.txt")
                logger.error("############################\n\n")
                raise RuntimeError("You need tiktoken or half of the functionality explodes my guy. Run 'uv pip install -r requirements.txt'")

            if args.count_tokens or args.split_tokens:
                if enc is None:
                    logger.error("\n\n############################")
                    logger.error("ERROR: You need tiktoken or half of the functionality explodes, my guy!")
                    logger.error("RERUN: uv pip install -r requirements.txt")
                    logger.error("############################\n\n")
                    raise RuntimeError("You need tiktoken or half of the functionality explodes my guy. Run 'uv pip install -r requirements.txt'")
                token_counter = get_token_counter(system_prompt_content, RESPONSE_FIELD, enc)
                token_counts = df.apply(token_counter, axis=1)
                total_tokens = token_counts.sum()
                avg_tokens = token_counts.mean()
                max_tokens = token_counts.max()
                logger.info(f"[TOKEN COUNT] Total: {total_tokens}, Avg: {avg_tokens:.1f}, Max: {max_tokens}")
                if args.split_tokens:
                    display_name = os.path.basename(resolved_path)
                    if total_tokens <= TOKEN_LIMIT:
                        logger.info(f"File {display_name} does not exceed the token limit. No split needed.")
                    else:
                        logger.info(f"Splitting {display_name} into chunks not exceeding {split_token_limit} tokens...")
                        output_files, token_counts = split_file_by_token_limit(resolved_path, split_token_limit, token_counter, RESPONSE_FIELD, output_dir=INPUT_DIR)
                        logger.info(f"Split complete. Output files: {output_files}")
                        for out_file, tok_count in zip(output_files, token_counts):
                            logger.info(f"Output file: {out_file} | Tokens: {tok_count}")
                continue
            ok = process_file(resolved_path, OUTPUT_DIR)
            if not ok:
                logger.error(f"[BATCH HALTED] Halting further batch processing due to failure in {resolved_path}.")
                break
        except Exception as e:
            logger.error(f"[BATCH HALTED] Error processing {resolved_path}: {e}")
            logger.error("Halting further batch processing due to failure.")
            break
    logger.success("Batch finished processing.\n")

    for handler in getattr(logger, 'file_logger', logging.getLogger()).handlers:
        try:
            handler.flush()
        except Exception:
            pass
        try:
            handler.close()
        except Exception:
            pass
    print("[CLEANUP] Logger handlers flushed and closed.")

    if show_stats:
        print_token_cost_stats()
