batch_options:
  max_simultaneous_batches: 1 # Keep it simple for the test
  halt_on_chunk_failure: false # Crucial setting
  # Default poll_interval, job_completion_timeout, etc.

llm_client_options:
  model: "gpt-3.5-turbo" # Will be mocked

input_splitter_options:
  max_rows_per_chunk: 2 # This will put ID 3 in its own chunk (chunk 2)
  # output_base_dir will be overridden in the test to use a temp directory

output_options:
  # output_base_dir and output_filename_template will be set in the test
  output_filename_template: "{original_filename}_results.csv" # Example

prompts: # Dummy prompts, content doesn't matter as LLM is mocked
  system_prompt: "You are a test assistant."
  user_prompt_template: "Test: {{text}}"
